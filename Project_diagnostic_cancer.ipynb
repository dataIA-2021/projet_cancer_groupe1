{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet - Application au diagnostic du cancer du sien "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptual Development of Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "Binary classificartion is the task of classifying a set into two groups on the basis of classification rule. Givr, a population whose members each belonging to one of a number of different sets, the elements of the population set are each predicted to belong to one another of the classes.\n",
    "\n",
    "The most popular algorithms used in binary classification are:\n",
    "\n",
    "1. Logistic Progression\n",
    "2. k-Nearest Neighbors\n",
    "3. Decision Trees\n",
    "4. Support Vector Machine\n",
    "5. Naive Bayes\n",
    "\n",
    "In this project; k-Nearest Neighbors(KNN) Algorithm will used. Logistic Regression and Support Vector Machines algorithms do not natively support more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Confusion\n",
    "\n",
    "It is a specific table lay-out that allows visualization of the performance of an algorithm, typically a supervised learning one, representing the summary of the prediction results on a classification problem(<a href=\"https://www.sciencedirect.com/topics/engineering/confusion-matrix\">Kulkarni, et.al, [2020]</a>).\n",
    "\n",
    "* `Each row` represents the instances in an actual class - `Actual` \n",
    "* `Each column` represnts the instances in a predicted class - `Predicted`\n",
    "\n",
    "Confusion matrix is an `N*N` matrix used for evaluating the performance of a classification model, where `N` is the number of target classes. The matrix compares the actual target values with those predicted by machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive vs. False Negative\n",
    "\n",
    "In a sample data from a population, a True Positive`(TP)` is an outcome where the model `correctly` predicts the Positive Class, while a False Negative(`FN`) is an outcome where the model `incorrectly` predicts the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<td></td>\n",
    "<td>Predicted Values</td>\n",
    "<thead>\n",
    "<tbody>\n",
    "</td></td>\n",
    "<tr>\n",
    "<td>Actual Values</td>\n",
    "<td>Positive</td>\n",
    "<td>Negative</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negative </td>\n",
    "<td>Number of True Psitives(TP)</td>\n",
    "<td>Number of False Negatives(FN)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negative </td>\n",
    "<td>Number of False Psitives(FP)</td>\n",
    "<td>Number of True Negatives(TN)</td>\n",
    "</body>\n",
    "</table>\n",
    "\n",
    "### Terminologies\n",
    "\n",
    "* <b>True Positive(TP):</b> when the actual value is Positive and predicted is also Positive\n",
    "* <b>True negatives(TN):</b> when the actual value is Negative and prediction is also Negative\n",
    "* <b>False Positives(FP):</b> also called <b>Type 1 error</b>, when the actual is negative but prediction is Positive.\n",
    "* <b>False Negatives(FN):</b> also called <b>Type 2 error</b>when the actual is Positive but the prediction is Negative.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy measures how often the classifier makes the correct prediction. It's the ratio between the number of correct predictions and the total number of predictions(<a href=\" https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\">Suresh[2020]</a>). It is a measure of correctness that is achieved in true predictions. In simple words, it tells us how many predictions are actually positive out of all the total positive predicted.\n",
    "\n",
    "Mathematically,\n",
    "$$\n",
    "Accuracy = \\frac {TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall is the ratio of correctly predicted values(`TP`) divided by total number of  actual values(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=In%20pattern%20recognition%2C%20information%20retrieval%20and%20Classification%20%28machine,amount%20of%20relevant%20instances%20that%20were%20actually%20retrieved.\">Wikipedia</a>). It recalls the correct prediction out of the total positive actual classes.\n",
    "\n",
    "Mathematically, recall is defined as:\n",
    "\n",
    "$$ \n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is a measure of correctness that is achieved in True observation. It tells how many predictions are actually positive out of the total positive predicted(<a href=\" https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\">Suresh[2020]</a>, <a href=\"https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/\">Huilgol[2020]</a>).\n",
    "\n",
    "Mathematically, \n",
    "\n",
    "$$\n",
    "Precision = \\frac {TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "Also called `F-score`, F1-score is a measure of a model's accuracy on a dataset(<a href=\"https://deepai.org/machine-learning-glossary-and-terms/f-score\">Wood</a>). It combines precision and recallof the model to measure the harmonic mean. Its value ranges between 0 and 1`(0 < F1-score < 1)`. This harmonic measurement is used to have an understanding when there is a case where there is no distiniction between the importance of precision or recall., hence they are combined.\n",
    "\n",
    "$$\n",
    "F1-Score = 2*\\frac {(Recall*Precision)}{(Recall + Precision)}\n",
    "$$\n",
    "*For a harmonous model, F-score should be high(ideally 1).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "The ROC (`Receiver operating Characteristic`) curve is a graphical plot that shows the performance of a machine learning model(<a href=\"https://medium.com/analytics-vidhya/what-is-roc-curve-1f776103c998\">Jadhav[2020]</a>). ROC Curve is needed to evaluate whether the model truely represents the dataset or not.  For example, the accuracy of the model implemented could produce high accuracy but it may fail to visualize the dataset in the real world-samples. Therefore, it is advisable to evaluate further. This is where the need of ROC curve comes. \n",
    "\n",
    "ROC curve is constrcucted by plotting the True Positive Rate(`TPR`). against the False Psitive Rate(`FPR`)(<a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">Wikipedia</a>). \n",
    "\n",
    "Mathematically, it is expressed by the equation given below:\n",
    "$$\n",
    "TPR = \\frac {TP}{TP + FN}\n",
    "$$\n",
    "These curves are importnt assistants in evaluating and fine-tuning classification models(<a href=\"https://towardsdatascience.com/demystifying-roc-curves-df809474529a\">Toshniwal[2020]</a>). Historically, ROC curve was developed in the 1940s by US Army to measure the ability of a radar's detective power  for incoming signals.\n",
    "\n",
    "#### The four main purpose of ROC Curve\n",
    "* Analysing the strength/predictive power of a model\n",
    "* Determing optimal threshold\n",
    "* Comparing two models\n",
    "\n",
    "For more detailed explanation about these purposes, please check an article written by <a href=\"https://towardsdatascience.com/demystifying-roc-curves-df809474529a\">Toshniwal[2020]</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
