{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet - Application au diagnostic du cancer du sien "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Development of Binary Classification Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concept of Binary Classification\n",
    "\n",
    "Binary classificartion is the task of classifying a set into two groups on the basis of classification rule. Givr, a population whose members each belonging to one of a number of different sets, the elements of the population set are each predicted to belong to one another of the classes.\n",
    "\n",
    "The most popular algorithms used in binary classification are:\n",
    "\n",
    "1. Logistic Progression\n",
    "2. k-Nearest Neighbors\n",
    "3. Decision Trees\n",
    "4. Support Vector Machine\n",
    "5. Naive Bayes\n",
    "\n",
    "In this project; k-Nearest Neighbors(KNN) Algorithm will used. Logistic Regression and Support Vector Machines algorithms do not natively support more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Confusion\n",
    "\n",
    "It is a specific table lay-out that allows visualization of the performance of an algorithm, typically a supervised learning one, representing the summary of the prediction results on a classification problem(<a href=\"https://www.sciencedirect.com/topics/engineering/confusion-matrix\">Kulkarni, et.al, [2020]</a>).\n",
    "\n",
    "* `Each row` represents the instances in an actual class - `Actual` \n",
    "* `Each column` represnts the instances in a predicted class - `Predicted`\n",
    "\n",
    "Confusion matrix is an `N*N` matrix used for evaluating the performance of a classification model, where `N` is the number of target classes. The matrix compares the actual target values with those predicted by machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive vs. False Negative\n",
    "\n",
    "In a sample data from a population, a True Positive`(TP)` is an outcome where the model `correctly` predicts the Positive Class, while a False Negative(`FN`) is an outcome where the model `incorrectly` predicts the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<td></td>\n",
    "<td>Predicted Values</td>\n",
    "<thead>\n",
    "<tbody>\n",
    "</td></td>\n",
    "<tr>\n",
    "<td>Actual Values</td>\n",
    "<td>Positive</td>\n",
    "<td>Negative</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negative </td>\n",
    "<td>Number of True Psitives(TP)</td>\n",
    "<td>Number of False Negatives(FN)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negative </td>\n",
    "<td>Number of False Psitives(FP)</td>\n",
    "<td>Number of True Negatives(TN)</td>\n",
    "</body>\n",
    "</table>\n",
    "\n",
    "### Terminologies\n",
    "\n",
    "* `True Positive(TP)`: when the actual value is Positive and predicted is also Positive\n",
    "* `True negatives(TN)`: when the actual value is Negative and prediction is also Negative\n",
    "* `False Positives(FP)`: also called `Type 1 error`, when the actual is negative but prediction is Positive.\n",
    "* `False Negatives(FN)`: also called `Type 2 error`when the actual is Positive but the prediction is Negative.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy measures how often the classifier makes the correct prediction. It's the ratio between the number of correct predictions and the total number of predictions(<a href=\" https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\">Suresh[2020]</a>). It is a measure of correctness that is achieved in true predictions. In simple words, it tells us how many predictions are actually positive out of all the total positive predicted.\n",
    "\n",
    "Mathematically,\n",
    "$$\n",
    "Accuracy = \\frac {TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall is the ratio of correctly predicted values(`TP`) divided by total number of  actual values(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=In%20pattern%20recognition%2C%20information%20retrieval%20and%20Classification%20%28machine,amount%20of%20relevant%20instances%20that%20were%20actually%20retrieved.\">Wikipedia</a>). It recalls the correct prediction out of the total positive actual classes.\n",
    "\n",
    "Mathematically, recall is defined as:\n",
    "\n",
    "$$ \n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is a measure of correctness that is achieved in True observation. It tells how many predictions are actually positive out of the total positive predicted(<a href=\" https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\">Suresh[2020]</a>, <a href=\"https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/\">Huilgol[2020]</a>).\n",
    "\n",
    "Mathematically, \n",
    "\n",
    "$$\n",
    "Precision = \\frac {TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "Also called `F-score`, F1-score is a measure of a model's accuracy on a dataset(<a href=\"https://deepai.org/machine-learning-glossary-and-terms/f-score\">Wood</a>). It combines precision and recallof the model to measure the harmonic mean. Its value ranges between 0 and 1`(0 < F1-score < 1)`. This harmonic measurement is used to have an understanding when there is a case where there is no distiniction between the importance of precision or recall., hence they are combined.\n",
    "\n",
    "$$\n",
    "F1-Score = 2*\\frac {(Recall*Precision)}{(Recall + Precision)}\n",
    "$$\n",
    "*For a harmonous model, F-score should be high(ideally 1).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC (Receiver Operating Characteristics) Curve\n",
    "\n",
    "The ROC (`Receiver operating Characteristic`) curve is a graphical plot that shows the performance of a machine learning model(<a href=\"https://medium.com/analytics-vidhya/what-is-roc-curve-1f776103c998\">Jadhav, 2020</a>). ROC Curve is needed to evaluate whether the model truely represents the dataset or not.  For example, the accuracy of the model implemented could produce high accuracy but it may fail to visualize the dataset in the real world-samples. Therefore, it is advisable to evaluate further. This is where the need of ROC curve comes. \n",
    "\n",
    "ROC curve is an evaluation for binary classification problems(<a href=\"https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\">Czakan, 2021</a>). It is constrcucted by plotting the True Positive Rate(`TPR`). against the False Positive Rate(`FPR`)(<a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">Wikipedia</a>). \n",
    "\n",
    "Mathematically, it is expressed by the equation given below:\n",
    "$$\n",
    "TPR = \\frac {TP}{TP + FN}\n",
    "$$\n",
    "$$\n",
    "FPR = \\frac {FP}{TN + FP}\n",
    "$$\n",
    "The ROC curve plots TPR(y-axis) vs. FPR(x-axis) at different classification thresholds(<a href=\"https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\">Bhandari, 2020</a>). The applied classifier threshold is inversely proportional to TPR and FPR, thus lowering the classification classifier increases both TPR and FPR. \n",
    "\n",
    "These curves are important assistants in evaluating and fine-tuning classification models(<a href=\"https://towardsdatascience.com/demystifying-roc-curves-df809474529a\">Toshniwal, 2020</a>). Historically, ROC curve was developed in the 1940s by US Army to measure the ability of a radar's detective power for incoming signals.\n",
    "\n",
    "##### The four main purpose of ROC Curve\n",
    "* Analysing the strength/predictive power of a model\n",
    "* Determing optimal threshold\n",
    "* Comparing two models\n",
    "\n",
    "For more detailed explanation about these purposes, please check an article written by <a href=\"https://towardsdatascience.com/demystifying-roc-curves-df809474529a\">Toshniwal, (2020)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC(Area under the ROC Curve)\n",
    "\n",
    "To compute the values in an ROC curve, the model applied should be evaluated many times with different classification thresholds. However, this is not sufficient enough bytiself. Therefore, an efficient sorting based algorithm is required to provide the information. and this is called AUC(\"<a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\">ROC Curve and AUC</a>\").\n",
    "\n",
    "AUC is the area under the ROC curve. To plot AUC plot, an an aggregate measurement of performance across all possible classification thresholds is computed(<a href=\"https://towardsdatascience.com/an-understandable-guide-to-roc-curves-and-auc-and-why-and-when-to-use-them-92020bc4c5c1\">Agarwal, 2021</a>). Its value ranges between `0 and 1`. The objective is to maximize AUC so that the highest TPR and lowest FPR values for some thresholds can be determined. A model whose predictions are 100% wrong has an `AUC` of 0.0; and one with 100% correct has ab AUC of 1.0. Thus, the choice of of the threshold depends on the ability of the model to balance between `False Positive` and `False Negative`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
